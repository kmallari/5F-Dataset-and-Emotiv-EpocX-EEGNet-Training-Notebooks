{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00fa5363",
   "metadata": {},
   "source": [
    "# Reading the .mat files from the 5F Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43132de",
   "metadata": {},
   "source": [
    "Add the necessary imports for preliminary steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ebe0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mne, glob\n",
    "import random\n",
    "import matplotlib\n",
    "import pathlib\n",
    "import random\n",
    "from scipy.io import loadmat\n",
    "from mne.io import concatenate_raws, read_raw_edf\n",
    "from itertools import chain\n",
    "from braindecode.datasets import create_from_X_y, BaseDataset, BaseConcatDataset\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from braindecode.preprocessing import exponential_moving_standardize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a70f49",
   "metadata": {},
   "source": [
    "Getting the files names of the .mat files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cbac84",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_files = glob.glob(\"*.mat\")\n",
    "mat_files = [fn for fn in mat_files if \"HFREQ\" not in fn]\n",
    "mat_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b50f5aa",
   "metadata": {},
   "source": [
    "Loads the data of the .mat files into the annots variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e699cccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "annots = [loadmat(fn) for fn in mat_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cd050b",
   "metadata": {},
   "source": [
    "Uncomment or modify the next lines to change the subjects to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da03c28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "annots = [annots[0]] # A\n",
    "# annots = [annots[2]] # B\n",
    "# annots = [annots[3]] # C\n",
    "# annots = [annots[4]] # F\n",
    "# annots = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570de317",
   "metadata": {},
   "source": [
    "### Merges the .mat files into one variable.\n",
    "- ```annots[i][\"o\"][0][0][5]``` contains the EEG signal values\n",
    "- ```annots[i][\"o\"][0][0][4]``` contains the events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e33e153",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = {\n",
    "    \"__header__\": \"Merged Files\",\n",
    "    \"__version__\": \"1.0\",\n",
    "    \"__globals__\": [],\n",
    "    \"o\": annots[-1][\"o\"],\n",
    "}\n",
    "\n",
    "for i in range(len(annots) - 1):\n",
    "    merged[\"o\"][0][0][5] = np.concatenate(\n",
    "        (merged[\"o\"][0][0][5], annots[i][\"o\"][0][0][5])\n",
    "    )\n",
    "    merged[\"o\"][0][0][4] = np.concatenate(\n",
    "        (merged[\"o\"][0][0][4], annots[i][\"o\"][0][0][4])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e995b95",
   "metadata": {},
   "source": [
    "# Extracting events and values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c7bbdd",
   "metadata": {},
   "source": [
    "Removes the unnecessary events in the events list. This leaves only classes `1`, `2`, `3`, `4`, `5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c684bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "events = [[1], [2], [3], [4], [5]]\n",
    "\n",
    "real_events = []\n",
    "real_vals = []\n",
    "\n",
    "for i, event in enumerate(merged[\"o\"][0][0][4]):\n",
    "    if event in events:\n",
    "        real_vals.append(merged[\"o\"][0][0][5][i])\n",
    "        real_events.append(merged[\"o\"][0][0][4][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22ed643",
   "metadata": {},
   "source": [
    "Flattens the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8ebf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_events = list(chain.from_iterable(real_events))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb66850a",
   "metadata": {},
   "source": [
    "Determines the indexes where there is a change in event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b487d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "left = 0\n",
    "non_repeating_events = []\n",
    "indexes = []\n",
    "\n",
    "real_events.append(-1)\n",
    "\n",
    "for right in range(1, len(real_events)):\n",
    "    if real_events[right] != real_events[left]:\n",
    "        indexes.append(left)  # appends the end; @ index 0, event = 1\n",
    "        non_repeating_events.append(real_events[left])\n",
    "    #         print(left, real_events[left])\n",
    "\n",
    "    left += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c644d7ba",
   "metadata": {},
   "source": [
    "Adds 0 to the start of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8395da",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = [0] + indexes\n",
    "# indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d733fa88",
   "metadata": {},
   "source": [
    "Gets the duration of each group of events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5f6cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "left = 0\n",
    "right = 1\n",
    "event_duration = []\n",
    "while right < len(indexes):\n",
    "    #     print(indexes[right] - indexes[left])\n",
    "    event_duration.append(indexes[right] - indexes[left])\n",
    "    right += 1\n",
    "    left += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91499d2e",
   "metadata": {},
   "source": [
    "Prints the information regarding the .mat files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38f6840",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_names = [channel[0][0] for channel in merged[\"o\"][0][0][6]]\n",
    "sfreq = merged[\"o\"][0][0][2][0][0]  # sfreq\n",
    "EEG_data = merged[\"o\"][0][0][5]\n",
    "EEG_data = np.transpose(EEG_data)\n",
    "n_channels, n_samples = EEG_data.shape\n",
    "event_codes = merged[\"o\"][0][0][4]\n",
    "cl_lab = [\"Thumb\", \"Index finger\", \"Middle finger\", \"Ring finger\", \"Pinkie finger\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808fc833",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"EEG_data shape: \", EEG_data.shape)\n",
    "print(\"Channels: \", channel_names)\n",
    "print(\"Sampling frequency: \", sfreq)\n",
    "print(\"Num of Samples: \", n_samples)\n",
    "print(\"Num of Channels: \", n_channels)\n",
    "print(\"Event codes: \", np.unique(event_codes))\n",
    "print(\"Class names: \", cl_lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b62023a",
   "metadata": {},
   "source": [
    "Concatenantes the EEG signal data and event codes. Also tranposes the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bf2f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "EEG_data = merged[\"o\"][0][0][5]\n",
    "EEG_concat = EEG_data\n",
    "EEG_concat = np.concatenate((EEG_data, event_codes), axis=1)\n",
    "EEG_concat = np.transpose(EEG_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce1499d",
   "metadata": {},
   "source": [
    "# Converting to mne.raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3516af3a",
   "metadata": {},
   "source": [
    "Creates the info variable from mne for the `raw` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e9ee01",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = mne.create_info(channel_names + [\"events\"], sfreq, \"eeg\")\n",
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c757d88",
   "metadata": {},
   "source": [
    "Converts the NumPy array to mne Raw object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70f2b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_raw = mne.io.RawArray(EEG_concat, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a425040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_raw.to_data_frame()  # Note that this multiplies the values by 1 million"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4552e8b1",
   "metadata": {},
   "source": [
    "# Converting to basedataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45a9705",
   "metadata": {},
   "source": [
    "Converts the `Raw` object to `basedataset` to preprocess the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d27430",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data = BaseConcatDataset([simulated_raw], target_transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f0cda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data = []\n",
    "for d in [simulated_raw]:\n",
    "    base_data.append(BaseDataset(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf81cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_base_data = BaseConcatDataset(base_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2399d29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_base_data.datasets[0].raw.__dict__[\"_data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63911a16",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1853210a",
   "metadata": {},
   "source": [
    "Does the necessary preprocessing to the `basedataset`. Specifically, the preprocessors used are channel reduction, bandpass filter, and exponential moving standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d2fbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from braindecode.preprocessing import (\n",
    "    exponential_moving_standardize,\n",
    "    preprocess,\n",
    "    Preprocessor,\n",
    "    scale,\n",
    ")\n",
    "\n",
    "# original\n",
    "low_cut_hz = 4.0  # low cut frequency for filtering\n",
    "high_cut_hz = 38.0  # high cut frequency for filtering\n",
    "\n",
    "# Parameters for exponential moving standardization\n",
    "factor_new = 1e-3\n",
    "init_block_size = 1000\n",
    "\n",
    "preprocessors = [\n",
    "    Preprocessor(\n",
    "        fn=\"pick_channels\",\n",
    "        ch_names=[\n",
    "            \"Fp1\",\n",
    "            \"Fp2\",\n",
    "            \"F3\",\n",
    "            \"F4\",\n",
    "            \"C3\",\n",
    "            \"C4\",\n",
    "            \"O1\",\n",
    "            \"O2\",\n",
    "            \"F7\",\n",
    "            \"F8\",\n",
    "            \"T3\",\n",
    "            \"T4\",\n",
    "            \"T5\",\n",
    "            \"T6\",\n",
    "        ],\n",
    "    ),  # Select 14 out 22 Channels\n",
    "    Preprocessor(\"filter\", l_freq=low_cut_hz, h_freq=high_cut_hz),  # Bandpass filter\n",
    "    Preprocessor(\"pick_types\", eeg=True, meg=False, stim=False),  # Keep EEG sensors\n",
    "    Preprocessor(\n",
    "        exponential_moving_standardize,  # Exponential moving standardization\n",
    "        factor_new=factor_new,\n",
    "        init_block_size=init_block_size,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Transform the data\n",
    "preprocess(concat_base_data, preprocessors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f93b8bf",
   "metadata": {},
   "source": [
    "Converts the `basedataset` to a pandas `dataframe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52c0095",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDF1 = [\n",
    "    d.raw.to_data_frame(scalings=dict(eeg=1, mag=1, grad=1))\n",
    "    for i, d in enumerate(concat_base_data.datasets)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46d4289",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDF1[0][\"events\"] = event_codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01301ef5",
   "metadata": {},
   "source": [
    "# Splitting Events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7527a30",
   "metadata": {},
   "source": [
    "Only gets 256 samples for each event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af62e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = []\n",
    "sum = 1\n",
    "\n",
    "for i, e in enumerate(event_duration):\n",
    "    times = e // 256\n",
    "    for j in range(times):\n",
    "        DATA.append(dataDF1[0][sum + 256 * j : sum + 256 + 256 * j])\n",
    "    sum += e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd958ba7",
   "metadata": {},
   "source": [
    "# Events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c78e141",
   "metadata": {},
   "source": [
    "Subtracts 1 to the value of the events.\n",
    "- 1 -> 0\n",
    "- 2 -> 1\n",
    "- 3 -> 2\n",
    "- 4 -> 3\n",
    "- 5 -> 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5470810",
   "metadata": {},
   "outputs": [],
   "source": [
    "events = []\n",
    "for d in DATA:\n",
    "    events.append(d[\"events\"][0:1].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd238c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Events, so Classes begins with 0\n",
    "events_newIndex = []\n",
    "for i, e in enumerate(events):\n",
    "    if e == 1:\n",
    "        events_newIndex.append(0)\n",
    "    elif e == 2:\n",
    "        events_newIndex.append(1)\n",
    "    elif e == 3:\n",
    "        events_newIndex.append(2)\n",
    "    elif e == 4:\n",
    "        events_newIndex.append(3)\n",
    "    elif e == 5:\n",
    "        events_newIndex.append(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e696a7e",
   "metadata": {},
   "source": [
    "# Create X Signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa16f68",
   "metadata": {},
   "source": [
    "Removes the `MarkerIndex` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa21587",
   "metadata": {},
   "outputs": [],
   "source": [
    "fourteen_channels = [\n",
    "    \"Fp1\",\n",
    "    \"Fp2\",\n",
    "    \"F3\",\n",
    "    \"F4\",\n",
    "    \"C3\",\n",
    "    \"C4\",\n",
    "    \"O1\",\n",
    "    \"O2\",\n",
    "    \"F7\",\n",
    "    \"F8\",\n",
    "    \"T3\",\n",
    "    \"T4\",\n",
    "    \"T5\",\n",
    "    \"T6\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6dc1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDF1 = [\n",
    "    d.raw.to_data_frame().loc[:, fourteen_channels]\n",
    "    for i, d in enumerate(concat_base_data.datasets)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6c5cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_5f = [d.loc[:, fourteen_channels] for d in DATA]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bca67cd",
   "metadata": {},
   "source": [
    "Tranposes the dataset to prepare for windowing later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8e7852",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, d in enumerate(data_5f):\n",
    "    data_5f[i] = data_5f[i].to_numpy().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4a07b0",
   "metadata": {},
   "source": [
    "# Reading our own data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31425c6",
   "metadata": {},
   "source": [
    "Since the Emotiv EpocX produceses both `.edf` and `.csv` files that contain essential information, both files are read.\n",
    "- `.edf` files contain the EEG signal values for the 14 channels.\n",
    "- `.csv` files contain the event codes that correspond to certain events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa10449d",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = pathlib.Path().resolve()  # gets current directory\n",
    "\n",
    "dataPaths = [\n",
    "    f\n",
    "    for f in listdir(current_path)\n",
    "    if isfile(join(current_path, f)) and f.split(\".\")[-1] == \"edf\"\n",
    "]\n",
    "markerFile = [\n",
    "    f\n",
    "    for f in listdir(current_path)\n",
    "    if isfile(join(current_path, f)) and f.split(\".\")[-1] == \"csv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6fe173",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    mne.io.read_raw_edf(path, preload=True, stim_channel=\"auto\") for path in dataPaths\n",
    "]\n",
    "\n",
    "markers = [pd.read_csv(path) for path in markerFile]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb39617f",
   "metadata": {},
   "source": [
    "Converts the data into a `basedataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bdbf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data = BaseConcatDataset(data, target_transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174cca9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data = []\n",
    "for d in data:\n",
    "    base_data.append(BaseDataset(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00025495",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_base_data = BaseConcatDataset(base_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9be492b",
   "metadata": {},
   "source": [
    "# Signal Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77708f5",
   "metadata": {},
   "source": [
    "Extracts the data from only the channels and store the values in `DataDF`. Also extracts the data from the `MarkerIndex` column and stores the data in `dataDF2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756841e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    mne.io.read_raw_edf(path, preload=True, stim_channel=\"auto\") for path in dataPaths\n",
    "]\n",
    "\n",
    "dataDF1 = [\n",
    "    d.raw.to_data_frame().loc[\n",
    "        :,\n",
    "        [\n",
    "            \"AF3\",\n",
    "            \"AF4\",\n",
    "            \"F3\",\n",
    "            \"F4\",\n",
    "            \"FC5\",\n",
    "            \"FC6\",\n",
    "            \"O1\",\n",
    "            \"O2\",\n",
    "            \"F7\",\n",
    "            \"F8\",\n",
    "            \"T7\",\n",
    "            \"T8\",\n",
    "            \"P7\",\n",
    "            \"P8\",\n",
    "        ],\n",
    "    ]\n",
    "    for i, d in enumerate(concat_base_data.datasets)\n",
    "]\n",
    "\n",
    "dataDF2 = [d.to_data_frame().loc[:, [\"MarkerIndex\"]] for i, d in enumerate(data)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b072d1",
   "metadata": {},
   "source": [
    "Preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbf9eac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "preprocessed = []\n",
    "for i, df in enumerate(dataDF1):\n",
    "    x = df.to_numpy() - 4200\n",
    "    x /= 10\n",
    "    bandpassed = mne.filter.filter_data(x, 256, l_freq=4, h_freq=38)\n",
    "    signal_standardized = exponential_moving_standardize(\n",
    "        bandpassed, factor_new=1e-3, init_block_size=1000\n",
    "    )\n",
    "    preprocessed.append(signal_standardized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f73d605",
   "metadata": {},
   "source": [
    "Removes the rest periods, which only keeps the motor imagery parts of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07465506",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDF = [pd.concat([dataDF1[i], dataDF2[i]], axis=1) for i, d in enumerate(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50eabe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_init = []\n",
    "temp = []\n",
    "# CONTAINS ODD NUMBERS FROM 9-21 (INCLUSIVE) AND THEIR NEGATIVE COUNTERPARTS\n",
    "# THESE ARE THE MARKERS FOR THE HAND MOVEMENTS\n",
    "pos_marker_vals = [i for i in range(9, 18) if i % 2 != 0]\n",
    "neg_marker_vals = [i for i in range(-17, -8) if i % 2 != 0]\n",
    "\n",
    "# print(pos_marker_vals)\n",
    "# print(neg_marker_vals)\n",
    "\n",
    "start = -1\n",
    "stop = -1\n",
    "\n",
    "# print(dataDF[0][\"MarkerIndex\"].unique())\n",
    "\n",
    "for i, df in enumerate(dataDF):\n",
    "    # temp = []\n",
    "    for j, row in df.iterrows():\n",
    "        # NEED THE .round() FUNCTION BECAUSE 15 AND 19\n",
    "        # ARE 14.99999 AND 18.99999 FOR SOME REASON\n",
    "        if row[\"MarkerIndex\"].round() in pos_marker_vals:\n",
    "            start = j\n",
    "        elif row[\"MarkerIndex\"].round() in neg_marker_vals:\n",
    "            stop = j\n",
    "\n",
    "        if start != -1 and stop != -1:\n",
    "            data_init.append(preprocessed[i][start : stop + 1])\n",
    "            start = -1\n",
    "            stop = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6582bdc9",
   "metadata": {},
   "source": [
    "Only gets the last 256 time samples of the data. The commented parts of the code block can be uncommented to retrieve more time samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e8061f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_epoch = []\n",
    "\n",
    "for i, df in enumerate(data_init):\n",
    "    size = len(df)\n",
    "    data_epoch.append(df[size - 256 : size])\n",
    "#     data_epoch.append(df[size-512:size-256])\n",
    "#     data_epoch.append(df[size-768:size-512])\n",
    "#     data_epoch.append(df[size-1024:size-768])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e8d451",
   "metadata": {},
   "source": [
    "Tranposes the data to prepare for windowing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd579bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, d in enumerate(data_epoch):\n",
    "    data_epoch[i] = data_epoch[i].transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf477c9a",
   "metadata": {},
   "source": [
    "# Markers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5408f538",
   "metadata": {},
   "source": [
    "Only select the `latency`,`duration`,`marker_value`, `marker_id` columns and only select the even numbered rows (Odd numbered rows are rest states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6f4ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, marker in enumerate(markers):\n",
    "    markers[i] = marker.loc[8:20, [\"latency\", \"duration\", \"marker_value\", \"marker_id\"]][\n",
    "        ::2\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f123b12",
   "metadata": {},
   "source": [
    "Appends the events to the `events` list. Uncomment the commented lines depending on how many time samples extracted earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547e8e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "events = []\n",
    "tempEvents = []\n",
    "\n",
    "for i, marker in enumerate(markers):\n",
    "    tempEvents = []\n",
    "    for index, row in markers[i].iterrows():\n",
    "        if row[\"marker_value\"] == \"hand-thumb\":\n",
    "            tempEvents.append(0)\n",
    "        #             tempEvents.append(0)\n",
    "        #             tempEvents.append(0)\n",
    "        #             tempEvents.append(0)\n",
    "        elif row[\"marker_value\"] == \"hand-index\":\n",
    "            tempEvents.append(1)\n",
    "        #             tempEvents.append(1)\n",
    "        #             tempEvents.append(1)\n",
    "        #             tempEvents.append(1)\n",
    "        elif row[\"marker_value\"] == \"hand-middle\":\n",
    "            tempEvents.append(2)\n",
    "        #             tempEvents.append(2)\n",
    "        #             tempEvents.append(2)\n",
    "        #             tempEvents.append(2)\n",
    "        elif row[\"marker_value\"] == \"hand-ring\":\n",
    "            tempEvents.append(3)\n",
    "        #             tempEvents.append(3)\n",
    "        #             tempEvents.append(3)\n",
    "        #             tempEvents.append(3)\n",
    "        elif row[\"marker_value\"] == \"hand-pinky\":\n",
    "            tempEvents.append(4)\n",
    "    #             tempEvents.append(4)\n",
    "    #             tempEvents.append(4)\n",
    "    #             tempEvents.append(4)\n",
    "    events.append(tempEvents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fac7d1",
   "metadata": {},
   "source": [
    "Flatten the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71b0233",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_list = [item for sublist in events for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbdcb57",
   "metadata": {},
   "source": [
    "# Merge 5F and Data from the Emotiv EpocX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53829af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_event = event_list + events_newIndex\n",
    "final_data = data_epoch + data_5f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f861590",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(final_event))\n",
    "print(len(final_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bf6e2f",
   "metadata": {},
   "source": [
    "# Windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6f0971",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_names = [\n",
    "    \"AF3\",\n",
    "    \"AF4\",\n",
    "    \"F3\",\n",
    "    \"F4\",\n",
    "    \"FC5\",\n",
    "    \"FC6\",\n",
    "    \"O1\",\n",
    "    \"O2\",\n",
    "    \"F7\",\n",
    "    \"F8\",\n",
    "    \"T7\",\n",
    "    \"T8\",\n",
    "    \"P7\",\n",
    "    \"P8\",\n",
    "]\n",
    "sfreq = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1410e56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "windows_dataset = create_from_X_y(\n",
    "    final_data,\n",
    "    final_event,\n",
    "    drop_last_window=False,\n",
    "    sfreq=sfreq,\n",
    "    ch_names=channel_names,\n",
    "    window_stride_samples=256,\n",
    "    window_size_samples=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54929a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_dataset.get_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9600c024",
   "metadata": {},
   "source": [
    "Selects the training and validation set with a 75/25 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01ba25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust baseed on Total Files\n",
    "session = [\"session_T\" for i in range(len(final_event))]\n",
    "\n",
    "valid_percent = round(len(final_event) * 0.25)\n",
    "num_of_valids = valid_percent // 5\n",
    "\n",
    "valids_count = [0 for i in range(5)]\n",
    "\n",
    "s = 0\n",
    "print(len(events))\n",
    "\n",
    "while s != num_of_valids * 5:\n",
    "    r = random.randint(0, len(final_event) - 1)\n",
    "    if valids_count[final_event[r] - 1] != num_of_valids:\n",
    "        session[r] = \"session_E\"\n",
    "        valids_count[final_event[r] - 1] += 1\n",
    "        s += 1\n",
    "\n",
    "description = pd.DataFrame({\"session\": session})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34e6409",
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_dataset.set_description(description)  # look as dataset description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6516c08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_dataset.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b45882",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted = windows_dataset.split(\"session\")\n",
    "train_set = splitted[\"session_T\"]\n",
    "valid_set = splitted[\"session_E\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41bab1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(train_set))\n",
    "# print(len(valid_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad6597d",
   "metadata": {},
   "source": [
    "# EEGNet Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecdc774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from braindecode.util import set_random_seeds\n",
    "from braindecode.models import ShallowFBCSPNet\n",
    "from braindecode.models import EEGNetv4\n",
    "\n",
    "cuda = torch.cuda.is_available()  # check if GPU is available, if True chooses to use it\n",
    "device = \"cuda\" if cuda else \"cpu\"\n",
    "if cuda:\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "# Set random seed to be able to roughly reproduce results\n",
    "# Note that with cudnn benchmark set to True, GPU indeterminism\n",
    "# may still make results substantially different between runs.\n",
    "# To obtain more consistent results at the cost of increased computation time,\n",
    "# you can set `cudnn_benchmark=False` in `set_random_seeds`\n",
    "# or remove `torch.backends.cudnn.benchmark = True`\n",
    "seed = 20200220\n",
    "set_random_seeds(seed=seed, cuda=cuda)\n",
    "\n",
    "n_classes = 5\n",
    "# Extract number of chans and time steps from dataset\n",
    "n_chans = train_set[0][0].shape[0]\n",
    "input_window_samples = train_set[0][0].shape[1]\n",
    "\n",
    "model = EEGNetv4(\n",
    "    n_chans,\n",
    "    n_classes,\n",
    "    input_window_samples=input_window_samples,\n",
    "    final_conv_length=\"auto\",\n",
    "    kernel_length=100,\n",
    ")\n",
    "\n",
    "# Send model to GPU\n",
    "if cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fce5767",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import LRScheduler\n",
    "from skorch.helper import predefined_split\n",
    "from braindecode import EEGClassifier\n",
    "\n",
    "# These values we found good for shallow network:\n",
    "lr = 0.0625 * 0.01\n",
    "weight_decay = 0\n",
    "\n",
    "batch_size = 64\n",
    "n_epochs = 500\n",
    "\n",
    "clf = EEGClassifier(\n",
    "    model,\n",
    "    criterion=torch.nn.NLLLoss,\n",
    "    optimizer=torch.optim.AdamW,\n",
    "    train_split=predefined_split(valid_set),  # using valid_set for validation\n",
    "    optimizer__lr=lr,\n",
    "    optimizer__weight_decay=weight_decay,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[\n",
    "        \"accuracy\",\n",
    "        (\"lr_scheduler\", LRScheduler(\"CosineAnnealingLR\", T_max=n_epochs - 1)),\n",
    "    ],\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Model training for a specified number of epochs. `y` is None as it is already supplied\n",
    "# in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6193f7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "clf.fit(train_set, y=None, epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e3903a",
   "metadata": {},
   "source": [
    "# Plotting and Matrix\n",
    "### Plots the loss and missclassification rate of the training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2ba7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import pandas as pd\n",
    "\n",
    "# Extract loss and accuracy values for plotting from history object\n",
    "results_columns = [\"train_loss\", \"valid_loss\", \"train_accuracy\", \"valid_accuracy\"]\n",
    "df = pd.DataFrame(\n",
    "    clf.history[:, results_columns],\n",
    "    columns=results_columns,\n",
    "    index=clf.history[:, \"epoch\"],\n",
    ")\n",
    "\n",
    "# get percent of misclass for better visual comparison to loss\n",
    "df = df.assign(\n",
    "    train_misclass=100 - 100 * df.train_accuracy,\n",
    "    valid_misclass=100 - 100 * df.valid_accuracy,\n",
    ")\n",
    "\n",
    "plt.style.use(\"seaborn\")\n",
    "fig, ax1 = plt.subplots(figsize=(20, 10))\n",
    "df.loc[:, [\"train_loss\", \"valid_loss\"]].plot(\n",
    "    ax=ax1, style=[\"-\", \":\"], color=\"tab:blue\", legend=False, fontsize=14\n",
    ")\n",
    "\n",
    "ax1.tick_params(axis=\"y\", labelcolor=\"tab:blue\", labelsize=14)\n",
    "ax1.set_ylabel(\"Loss\", color=\"tab:blue\", fontsize=14)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "df.loc[:, [\"train_misclass\", \"valid_misclass\"]].plot(\n",
    "    ax=ax2, style=[\"-\", \":\"], color=\"tab:red\", legend=False\n",
    ")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=\"tab:red\", labelsize=14)\n",
    "ax2.set_ylabel(\"Misclassification Rate [%]\", color=\"tab:red\", fontsize=14)\n",
    "ax2.set_ylim(ax2.get_ylim()[0], 85)  # make some room for legend\n",
    "ax1.set_xlabel(\"Epoch\", fontsize=14)\n",
    "\n",
    "# where some data has already been plotted to ax\n",
    "handles = []\n",
    "handles.append(\n",
    "    Line2D([0], [0], color=\"black\", linewidth=1, linestyle=\"-\", label=\"Train\")\n",
    ")\n",
    "handles.append(\n",
    "    Line2D([0], [0], color=\"black\", linewidth=1, linestyle=\":\", label=\"Valid\")\n",
    ")\n",
    "plt.legend(handles, [h.get_label() for h in handles], fontsize=14)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4897e443",
   "metadata": {},
   "source": [
    "### Generates the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6570257e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from braindecode.visualization import plot_confusion_matrix\n",
    "\n",
    "# generate confusion matrices\n",
    "# get the targets\n",
    "y_true = valid_set.get_metadata().target\n",
    "y_pred = clf.predict(valid_set)\n",
    "\n",
    "# generating confusion matrix\n",
    "confusion_mat = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# add class labels\n",
    "# label_dict is class_name : str -> i_class : int\n",
    "# label_dict = valid_set.datasets[0].windows.event_id.items()\n",
    "\n",
    "label_dict = {\n",
    "    \"Thumb\": 0,\n",
    "    \"Index\": 1,\n",
    "    \"Middle\": 2,\n",
    "    \"Ring\": 3,\n",
    "    \"Pinky\": 4,\n",
    "}.items()\n",
    "\n",
    "# sort the labels by values (values are integer class labels)\n",
    "labels = list(dict(sorted(list(label_dict), key=lambda kv: kv[1])).keys())\n",
    "\n",
    "# plot the basic conf. matrix\n",
    "plot_confusion_matrix(confusion_mat, class_names=labels, with_f1_score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7380e86",
   "metadata": {},
   "source": [
    "# Saving the model\n",
    "Uncomment to save your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c7204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# torch.save(model.state_dict(), \"file_name.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
